{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.linalg as la\n",
    "import itertools as it\n",
    "import time\n",
    "import pylab as pl\n",
    "import random\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import lognorm\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test\n",
    "def noisysincfunction(N, noise):\n",
    "    ''' noisysincfunction - generate data from the \"noisy sinc function\"\n",
    "        % usage\n",
    "        %     [X, Y] = noisysincfunction(N, noise)\n",
    "        %\n",
    "        % input\n",
    "        %     N: number of data points\n",
    "        %     noise: standard variation of the noise\n",
    "        %\n",
    "        % output\n",
    "        %     X: (1, N)-matrix uniformly sampled in -2pi, pi\n",
    "        %     Y: (1, N)-matrix equal to sinc(X) + noise\n",
    "        %\n",
    "        % description\n",
    "        %     Generates N points from the noisy sinc function\n",
    "        %\n",
    "        %        X ~ uniformly in [-2pi, pi]\n",
    "        %        Y = sinc(X) + eps, eps ~ Normal(0, noise.^2)\n",
    "        %\n",
    "        % author\n",
    "        %     Mikio Braun\n",
    "    '''\n",
    "    X = np.sort(2 * np.pi * np.random.rand(1, N) ) - np.pi\n",
    "    Y = np.sinc(X) + noise * np.random.randn(1, N)\n",
    "    return X.reshape(-1, 1), Y.flatten()\n",
    "Xtr, Ytr = noisysincfunction(100, 0.1)\n",
    "Xte = np.arange( -np.pi, np.pi, 0.01 ).reshape(-1, 1)\n",
    "pl.plot(Xtr,Ytr, 'o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_one_loss(y_true, y_pred):\n",
    "    ''' your header here!\n",
    "    '''\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_error(y_true, y_pred):\n",
    "    ''' \n",
    "    your code here \n",
    "    '''\n",
    "    loss = np.mean(abs(y_pred-y_true))\n",
    "    #loss = np.sum(np.sum((y_pred-y_true)**2)**0.5, axis = 1) / len(y_pred)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv(X, y, method, params, loss_function=mean_absolute_error, nfolds=10, nrepetitions=5):\n",
    "    ''' \n",
    "    your header here!\n",
    "    '''\n",
    "    # TODO progress bar, run time estimation\n",
    "    length, width = np.shape(X)\n",
    "    # model = method\n",
    "    method.cvloss = 1000000\n",
    "    params_combinations = list(it.product(params['kernel'], params['kernelparameter'], params['regularization']))\n",
    "    for parameter in tqdm(params_combinations):\n",
    "        #print(parameter[1])\n",
    "        model = method(parameter[0], parameter[1], parameter[2])\n",
    "        e = 0\n",
    "        for i in range(nrepetitions):\n",
    "            # Random Partitioning\n",
    "            X_pos = np.linspace(0,length-1, length)\n",
    "            random.shuffle(X_pos)\n",
    "            part = np.array_split(X_pos, nfolds)\n",
    "            for j in range(nfolds):\n",
    "                # Assign every part not j as training set\n",
    "                # Xtr indices\n",
    "                train = np.concatenate(np.array(part)[tuple([np.array(range(nfolds)) != j])].astype('int')) \n",
    "                X_j = X[train]\n",
    "                y_j = y[train]\n",
    "                model.fit(X_j, y_j)\n",
    "                y_pred = model.predict(X[part[j].astype('int')])\n",
    "                e = e + loss_function(y[part[j].astype('int')], y_pred)\n",
    "        e = e / (nfolds * nrepetitions)\n",
    "        #print('Loss:' + str(model.cvloss))\n",
    "        if e < method.cvloss:\n",
    "            #print(e)\n",
    "            method.cvloss = e\n",
    "            #print('Loss in if:' + str(model.cvloss))\n",
    "            method.__params = parameter\n",
    "    #print(method.params)\n",
    "    method = model.fit(X,y,method.__params[0],method.__params[1],method.__params[2])\n",
    "    return method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class krr():\n",
    "    ''' your header here!\n",
    "    '''\n",
    "    def __init__(self, kernel='linear', kernelparameter=1, regularization=0):\n",
    "        self.kernel = kernel\n",
    "        self.kernelparameter = kernelparameter\n",
    "        self.regularization = regularization\n",
    "\n",
    "    def fit(self, X, y, kernel=False, kernelparameter=False, regularization=False):\n",
    "        ''' your header here!\n",
    "        '''\n",
    "        self.__Xtrain = X\n",
    "        self.__ytrain = y\n",
    "        \n",
    "        if kernel is not False:\n",
    "            self.kernel = kernel\n",
    "        if kernelparameter is not False:\n",
    "            self.kernelparameter = kernelparameter\n",
    "        if regularization is not False:\n",
    "            self.regularization = regularization\n",
    "        # calculate kernelmatrix\n",
    "        if self.kernel == 'linear':\n",
    "            self.__linearKernel(X)\n",
    "        elif self.kernel == 'polynomial':\n",
    "            self.__polynomialKernel(X)\n",
    "        elif self.kernel == 'gaussian':\n",
    "            self.__gaussianKernel(X)\n",
    "        else:\n",
    "            print(\"\"\"The following kernel {} is not known. Please use either 'linear' , 'polynomial' or 'gaussian'.\"\"\".format(kernel))\n",
    "        if self.regularization == 0:\n",
    "            self.__LOOCV()\n",
    "        \n",
    "        # calculate optimized alpha\n",
    "        I_length = len(self.kernelmatrix)\n",
    "        self.alpha = np.linalg.solve(self.kernelmatrix+self.regularization*np.identity(I_length), self.__ytrain).reshape(-1,len(self.__ytrain))\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        ''' your header here!\n",
    "        '''\n",
    "        # calculate kernelmatrix\n",
    "        if self.kernel == 'linear':\n",
    "            self.__linearKernel(X)\n",
    "        elif self.kernel == 'polynomial':\n",
    "            self.__polynomialKernel(X)\n",
    "        elif self.kernel == 'gaussian':\n",
    "            self.__gaussianKernel(X)\n",
    "        # calculate prediction\n",
    "        y_pred = self.alpha.dot(self.kernelmatrix) # <alpha,kernelmatrix>\n",
    "        return y_pred.reshape(len(X),len(self.alpha))\n",
    "    \n",
    "    def __linearKernel(self,Y):\n",
    "        self.kernelmatrix = self.__Xtrain.dot(Y.T)\n",
    "        \n",
    "    def __polynomialKernel(self,Y):\n",
    "        self.kernelmatrix = (self.__Xtrain.dot(Y.T)+1)**self.kernelparameter\n",
    "        \n",
    "    def __gaussianKernel(self,Y):\n",
    "        X_len, X_width = self.__Xtrain.shape\n",
    "        self.kernelmatrix = np.exp(-(np.diagonal(self.__Xtrain.dot(self.__Xtrain.T)).reshape(X_len, 1)-2*self.__Xtrain.dot(Y.T)+np.diagonal(Y.dot(Y.T)))/(2*(self.kernelparameter**2)))\n",
    "        \n",
    "    def __LOOCV(self):\n",
    "        # Leave-One-Out-Cross-Validation\n",
    "        # starting squared error\n",
    "        #min_squared_loss = 1000\n",
    "        # Eigenvalue decomposition\n",
    "        squared_loss = []\n",
    "        L, U = np.linalg.eigh(self.kernelmatrix) # L = Eigenvalue, U = Eigenvector\n",
    "        mean_L = np.mean(L)\n",
    "        I = np.identity(len(L))\n",
    "        # for faster computation precalculate U.T.y\n",
    "        UTy = U.T.dot(self.__ytrain)\n",
    "        # logarithmic distribution with mu = mean_L and sigma = 1\n",
    "        # create 50 values of C\n",
    "        # identify C around Kernel eigenvalue means with logarithmic distribution \n",
    "        Cs = np.logspace(-10, 10, 50)*mean_L\n",
    "        for C in Cs: #np.random.lognormal(mean_L,sigma = 1, size = 30):\n",
    "            ULCI = U.dot(L*I).dot((1/(L + C))*I) # (1/(L + C))*I: inverse of diagonal matrix\n",
    "            squared_loss.append(np.sum(((self.__ytrain - ULCI.dot(UTy)) / (1-np.diagonal(ULCI.dot(U.T))))**2) / len(self.__ytrain))\n",
    "        self.regularization = Cs[np.argmin(squared_loss)]\n",
    "                \n",
    "        # Eigenvalue decomposition\n",
    "        #L, U = np.linalg.eigh(self.kernelmatrix) # L = Eigenvalue, U = Eigenvector\n",
    "        #mean_L = np.mean(L)        \n",
    "                \n",
    "        # create candidates\n",
    "        #C = np.logspace(-10, 10, 50)*mean_L\n",
    "        \n",
    "        #UTy = U.T.dot(self.__ytrain)\n",
    "        # \n",
    "        #UL = U.dot(np.diag(L))[None,:,:]\n",
    "        #LC = np.diag(L)[None,:,:]+C[:,None,None]*I[None,:,:]\n",
    "        #ULCI = np.linalg.solve(LC,UL)\n",
    "        \n",
    "        #calculate all quadratic losses for c candidates\n",
    "        #c   =    0xnx1-cxnx1 / (1-cxn)\n",
    "        #epsilon = (np.sum(((Ytr - ULCI.dot(UTy)) / (1-np.diagonal(ULCI.dot(U.T))).reshape(len(C),len(Ytr)))**2, axis = 1) /len(Ytr))\n",
    "    \n",
    "        #best_epsilon = epsilon[np.argmin(epsilon)]\n",
    "    \n",
    "        #self.regularization = C[np.argmin(epsilon)]        \n",
    "        \n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Xtr, Ytr = noisysincfunction(100, 0.1)\n",
    "Xte = np.arange( -np.pi, np.pi, 0.01 ).reshape(-1, 1)\n",
    "\n",
    "pl.figure()\n",
    "kernels = ['gaussian','polynomial','linear']\n",
    "titles = ['gaussian','polynomial','linear']\n",
    "params = [0.5,6,0]\n",
    "regularizations = [ 0.01,0.01,0.01]\n",
    "for i in range(3):\n",
    "    for j in range(2):\n",
    "        pl.subplot(2,3,1+i+3*j)\n",
    "        if j==0:\n",
    "            model = krr(kernel=kernels[i],\n",
    "                kernelparameter=params[i],\n",
    "                regularization=regularizations[i])\n",
    "            model.fit(Xtr,Ytr)\n",
    "        if j==1:\n",
    "            model = krr(kernel=kernels[i],\n",
    "                kernelparameter=params[i],\n",
    "                regularization=0)\n",
    "            model.fit(Xtr,Ytr)\n",
    "        ypred = model.predict(Xte)\n",
    "        pl.plot(Xtr,Ytr)\n",
    "        pl.plot(Xte,ypred)\n",
    "        if j==0 and i == 0:\n",
    "            pl.ylabel('fixed regularization')\n",
    "        if j==1 and i == 0:\n",
    "            pl.ylabel('reg. by efficent cv')\n",
    "        pl.title( titles[i] )\n",
    "pl.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Xtr, Ytr = noisysincfunction(100, 0.1)\n",
    "Xte = np.arange( -np.pi, np.pi, 0.01 ).reshape(-1, 1)\n",
    "\n",
    "pl.figure()\n",
    "kernels = ['gaussian','polynomial','linear']\n",
    "titles = ['gaussian','polynomial','linear']\n",
    "params = [0.5,6,0]\n",
    "regularizations = [ 0.01,0.01,0.01]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model = krr(kernel=kernels[0],\n",
    "                kernelparameter=params[0],\n",
    "                regularization=regularizations[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.fit(Xtr,Ytr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.kernelmatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.kernelmatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.alpha.dot(model.kernelmatrix.reshape(len(Xtr), len(Xtr)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ytr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.predict(Xte)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I_length = len(model.kernelmatrix)\n",
    "#self.alpha = np.linalg.solve(self.kernelmatrix+self.regularization*np.identity(I_length), self.__ytrain).reshape(-1,len(self.__ytrain))\n",
    "y_pred = (model.kernelmatrix+model.regularization*np.identity(I_length)).dot(model.alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "float(y_pred[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "float(Ytr[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Xtr, Ytr = noisysincfunction(100, 0.1)\n",
    "Xte = np.arange( -np.pi, np.pi, 0.01 ).reshape(-1, 1)\n",
    "\n",
    "pl.figure()\n",
    "pl.subplot(1,2,1)\n",
    "params = { 'kernel': ['gaussian'], 'kernelparameter': np.logspace(-4,4,20), 'regularization': np.logspace(-2,2,10) }\n",
    "cvkrr = cv(Xtr, Ytr, krr, params, loss_function=mean_absolute_error, nrepetitions=2)\n",
    "ypred = cvkrr.predict(Xte)\n",
    "print('Regularization range: 10**-4 .. 10**4')\n",
    "print('Gaussian kernel parameter: ', cvkrr.kernelparameter)\n",
    "print('Regularization paramter: ', cvkrr.regularization)\n",
    "\n",
    "pl.plot(Xtr,Ytr)\n",
    "pl.plot(Xte,ypred)\n",
    "\n",
    "pl.subplot(1,2,2)\n",
    "params = { 'kernel': ['gaussian'], 'kernelparameter': np.logspace(-2,2,10), 'regularization': [0]}\n",
    "cvkrr = cv(Xtr, Ytr, krr, params, loss_function=mean_absolute_error, nrepetitions=2)\n",
    "ypred = cvkrr.predict(Xte)\n",
    "print('Regularization via efficient leave on out')\n",
    "print('Kernel parameter: ', cvkrr.kernelparameter)\n",
    "print('Regularization paramter: ', cvkrr.regularization)\n",
    "\n",
    "pl.plot(Xtr,Ytr)\n",
    "pl.plot(Xte,ypred)\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
